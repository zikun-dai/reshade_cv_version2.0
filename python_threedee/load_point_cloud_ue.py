#!/usr/bin/env python3
import os
import argparse
import json
import math
import numpy as np
from PIL import Image
from save_point_cloud_to_file import save_cloud_to_file
from misc_utils import files_glob
from functools import partial
from tqdm.contrib.concurrent import process_map

# -------------------------- æ ¸å¿ƒå·¥å…·å‡½æ•°ï¼ˆä¸æ­£ç¡®è„šæœ¬å¯¹é½ï¼‰ --------------------------
def d2r(x):
    return x * math.pi / 180.0

def make_K_from_fovy(fovy_deg, W, H, aspect_ratio=None):
    """ç”¨å‚ç›´FOVè®¡ç®—å†…å‚ï¼ˆä¸æ­£ç¡®è„šæœ¬é€»è¾‘ä¸€è‡´ï¼‰"""
    if aspect_ratio is None:
        aspect_ratio = W / H
    fovy = d2r(fovy_deg)
    fy = (H * 0.5) / math.tan(fovy * 0.5)
    fovx = 2.0 * math.atan(aspect_ratio * math.tan(fovy * 0.5))  # è½¬æ¢ä¸ºæ°´å¹³FOV
    fx = (W * 0.5) / math.tan(fovx * 0.5)
    cx = (W - 1) / 2.0
    cy = (H - 1) / 2.0
    return fx, fy, cx, cy

def make_K_from_fovx(fovx_deg, W, H, aspect_ratio=None):
    if aspect_ratio is None:
        aspect_ratio = W / H
    fovx = d2r(fovx_deg)
    fx = (W * 0.5) / math.tan(fovx * 0.5)
    v = 2.0 * math.atan(math.tan(fovx * 0.5) / aspect_ratio)
    fy = (H * 0.5) / math.tan(v * 0.5)
    cx = (W - 1) / 2.0
    cy = (H - 1) / 2.0
    return fx, fy, cx, cy
def backproject_points_from_z_depth(depth, fx, fy, cx, cy, stride=1):
    """åƒç´ â†’ç›¸æœºç³»åæŠ•å½±ï¼ˆä¸æ­£ç¡®è„šæœ¬ä¸€è‡´ï¼‰"""
    H, W = depth.shape[:2]
    us = np.arange(0, W, stride)
    vs = np.arange(0, H, stride)
    uu, vv = np.meshgrid(us, vs)
    
    z = depth[::stride, ::stride]
    x = (uu - cx) * z / fx
    y = (vv - cy) * z / fy
    
    pts_cam = np.stack([x, -y, -z], axis=-1).reshape(-1, 3)
    return pts_cam, uu.reshape(-1), vv.reshape(-1)

# -------------------------- ä»extrinsic_cam2worldè§£æUEâ†’OpenCVè½¬æ¢ --------------------------
# def cam2world_to_cv(cam2world, pose_scale=1.0):
#     """
#     å°†UEçš„3x4ç›¸æœºçŸ©é˜µè½¬æ¢ä¸ºOpenCVç³»c2wçŸ©é˜µ
#     cam2world: 3x4æ•°ç»„ï¼Œæ ¼å¼ä¸º[R_ue(3x3) | t_ue(3x1)]
#     """
#     # 1. æå–UEç³»æ—‹è½¬å’Œå¹³ç§»
#     R_ue = cam2world[:, :3]  # 3x3æ—‹è½¬çŸ©é˜µï¼ˆUEç³»ï¼‰
#     t_ue = cam2world[:, 3]   # 3x1å¹³ç§»å‘é‡ï¼ˆUEç³»ï¼Œæœªç¼©æ”¾ï¼‰
    
#     # 2. åº”ç”¨ç¼©æ”¾ï¼ˆä¸æ­£ç¡®è„šæœ¬çš„pose_scaleä¸€è‡´ï¼‰
#     t_scaled = t_ue * pose_scale
    
#     # 3. UEâ†’OpenCVè½´æ˜ å°„ï¼ˆä¸æ­£ç¡®è„šæœ¬çš„M_to_CVä¸€è‡´ï¼‰
#     M_to_CV = np.array([[0, 1,  0],
#                            [0, 0, -1],
#                            [1, 0,  0]], dtype=np.float64)
#     # æ—‹è½¬çŸ©é˜µè½¬æ¢
#     R_cv = M_to_CV @ R_ue @ M_to_CV.T
#     # å¹³ç§»å‘é‡è½¬æ¢ï¼ˆåˆ†é‡å¯¹åº”ä¸æ­£ç¡®è„šæœ¬ä¸€è‡´ï¼‰
#     t_cv = np.array([
#         t_scaled[2],  # OpenCV X = UE Xï¼ˆå‰æ–¹å‘ï¼‰
#         t_scaled[1],  # OpenCV Y = UE Zï¼ˆä¸Šæ–¹å‘ï¼‰
#         t_scaled[0]   # OpenCV Z = UE Yï¼ˆå³æ–¹å‘ï¼‰
#     ], dtype=np.float64)
    
#     # 4. æ„é€ 4x4 c2wçŸ©é˜µ
#     c2w = np.eye(4, dtype=np.float64)
#     c2w[:3, :3] = R_cv
#     c2w[:3, 3] = t_cv
#     return c2w, R_cv, t_cv

def cam2world_to_cv_unchanged(cam2world, pose_scale=1.0):
    """
    å°†UEçš„3x4ç›¸æœºçŸ©é˜µè½¬æ¢ä¸ºOpenCVç³»c2wçŸ©é˜µ
    cam2world: 3x4æ•°ç»„ï¼Œæ ¼å¼ä¸º[R_ue(3x3) | t_ue(3x1)]
    """
    # 1. æå–UEç³»æ—‹è½¬å’Œå¹³ç§»
    R_cv = cam2world[:, :3]  # 3x3æ—‹è½¬çŸ©é˜µï¼ˆUEç³»ï¼‰
    t_cv = cam2world[:, 3]   # 3x1å¹³ç§»å‘é‡ï¼ˆUEç³»ï¼Œæœªç¼©æ”¾ï¼‰

    R_cv_new = R_cv.copy()

    R_cv_new[:,0] = R_cv[:,0]
    R_cv_new[:,1] = R_cv[:,2]
    R_cv_new[:,2] = -R_cv[:,1]

    # 4. æ„é€ 4x4 c2wçŸ©é˜µ
    c2w = np.eye(4, dtype=np.float64)
    c2w[:3, :3] = R_cv
    c2w[:3, 3] = t_cv

    return c2w, R_cv, t_cv


# -------------------------- åŠ è½½æ·±åº¦å’Œç›¸æœºæ–‡ä»¶ï¼ˆä¼˜å…ˆcamera.jsonï¼Œå†æ‰¾meta.jsonï¼‰ --------------------------
def load_depth_and_meta(depthfile:str, and_rgb:bool):
    """é€‚é…é€»è¾‘ï¼šä¼˜å…ˆæŸ¥æ‰¾camera.jsonï¼Œä¸å­˜åœ¨åˆ™æŸ¥æ‰¾meta.jsonï¼Œå‡éœ€å«extrinsic_cam2worldå’Œfov_h_degrees"""
    # 1. è§£ææ·±åº¦æ–‡ä»¶
    if depthfile.endswith('_depth.npy'):
        depthbnam = depthfile[:-len('_depth.npy')]
        depth = np.load(depthfile, allow_pickle=False)
    elif depthfile.endswith('_depth.fpzip'):
        depthbnam = depthfile[:-len('_depth.fpzip')]
        import fpzip
        with open(depthfile,'rb') as infile:
            depth = fpzip.decompress(infile.read())
        if len(depth.shape) == 4:
            depth = depth[0,0,:,:]
    else:
        print(f"[è­¦å‘Š] ä¸æ”¯æŒçš„æ·±åº¦æ ¼å¼: {depthfile}")
        return None
    
    assert depth.dtype in (np.float32, np.float64), f"æ·±åº¦æ•°æ®ç±»å‹é”™è¯¯: {depth.dtype}"
    assert len(depth.shape) == 2, f"æ·±åº¦ç»´åº¦é”™è¯¯: {depth.shape}"

    # 2. ä¼˜å…ˆæŸ¥æ‰¾camera.json
    cam_file = depthbnam + '_camera.json'
    if os.path.isfile(cam_file):
        print(f"[DEBUG] æ‰¾åˆ°camera.json: {cam_file}")
        with open(cam_file,'r') as f:
            cam_data = json.load(f)
    else:
        # camera.jsonä¸å­˜åœ¨ï¼ŒæŸ¥æ‰¾meta.json
        print(f"[DEBUG] æœªæ‰¾åˆ°camera.json: {cam_file}ï¼Œå°è¯•æŸ¥æ‰¾meta.json")
        cam_file = depthbnam + '_meta.json'
        if not os.path.isfile(cam_file):
            print(f"[è­¦å‘Š] æœªæ‰¾åˆ°camera.jsonå’Œmeta.json: {depthbnam}_camera.json / {depthbnam}_meta.json")
            return None
        print(f"[DEBUG] æ‰¾åˆ°meta.json: {cam_file}")
        with open(cam_file,'r') as f:
            cam_data = json.load(f)
    
    # 3. éªŒè¯ç›¸æœºæ–‡ä»¶å¿…è¦å­—æ®µï¼ˆä¸¤ç§æ–‡ä»¶ç»Ÿä¸€éªŒè¯æ ‡å‡†ï¼‰
    required_keys = ['extrinsic_cam2world', 'fov_h_degrees']
    for k in required_keys:
        if k not in cam_data:
            print(f"[è­¦å‘Š] {os.path.basename(cam_file)}ç¼ºå°‘å­—æ®µ'{k}': {sorted(cam_data.keys())}")
            return None

    # 4. è¯»å–RGB
    if and_rgb:
        rgb_file = depthbnam + '_RGB.png'
        if not os.path.isfile(rgb_file):
            print(f"[è­¦å‘Š] æœªæ‰¾åˆ°RGBæ–‡ä»¶: {rgb_file}")
            return None
        rgb = np.asarray(Image.open(rgb_file).convert('RGB'))
        assert rgb.shape[:2] == depth.shape[:2], f"RGBä¸æ·±åº¦å°ºå¯¸ä¸åŒ¹é…: {rgb.shape} vs {depth.shape}"
        return depth, cam_data, rgb, depthbnam
    return depth, cam_data, None, depthbnam

# -------------------------- ç”Ÿæˆç‚¹äº‘ï¼ˆæ ¸å¿ƒé€»è¾‘ä¸å˜ï¼‰ --------------------------
def load_cloud_via_meta(depthfile:str,
            colored:bool,
            max_distance:float=None,
            subsample_amt:int=0,
            pose_scale:float=1.0,
            ):
    # åŠ è½½æ·±åº¦å’Œç›¸æœºæ•°æ®ï¼ˆä¼˜å…ˆcamera.jsonï¼‰
    result = load_depth_and_meta(depthfile, colored)
    if result is None:
        return None
    if colored:
        depth, cam_data, rgb, depthbnam = result
        rgb_image = np.copy(rgb)
    else:
        depth, cam_data, _, depthbnam = result
    depth_image = np.copy(depth)
    H, W = depth.shape[:2]
    aspect_ratio = W / H

    # 1. ä»ç›¸æœºæ•°æ®è¯»å–å‚æ•°ï¼ˆcamera.jsonå’Œmeta.jsonç»“æ„ä¸€è‡´ï¼‰
    fov_h_deg = float(cam_data['fov_h_degrees'])  # å‚ç›´FOV
    cam2world = np.array(cam_data['extrinsic_cam2world'], dtype=np.float64).reshape(3, 4)  # 3x4ç›¸æœºçŸ©é˜µ
    print("cam2world:\n", cam2world)
    # 2. è½¬æ¢ä¸ºOpenCVç³»c2wçŸ©é˜µï¼ˆä¸æ­£ç¡®è„šæœ¬å¯¹é½ï¼‰
    c2w, R_cv, t_cv = cam2world_to_cv_unchanged(cam2world, pose_scale)
    # t_cv *= 10
    print(f"[DEBUG] å¸§ {depthbnam} çš„c2wçŸ©é˜µ:\n{c2w}")

    # 3. è®¡ç®—å†…å‚ï¼ˆç”¨å‚ç›´FOVï¼Œä¸æ­£ç¡®è„šæœ¬é€»è¾‘ä¸€è‡´ï¼‰
    fx, fy, cx, cy = make_K_from_fovx(fov_h_deg, W, H, aspect_ratio)
    print(f"[DEBUG] å†…å‚: fx={fx:.2f}, fy={fy:.2f}, cx={cx:.2f}, cy={cy:.2f}")

    # 4. ç‚¹äº‘åæŠ•å½±
    pts_cam, uu, vv = backproject_points_from_z_depth(depth, fx, fy, cx, cy, stride=1)
    # æ·±åº¦è£å‰ªï¼ˆä¸æ­£ç¡®è„šæœ¬ä¸€è‡´ï¼‰
    depth_flat = depth[vv, uu]
    depth_mask_keep = (depth_flat >= 0.2) & (depth_flat <= max_distance)
    pts_cam = pts_cam[depth_mask_keep]
    uu_keep = uu[depth_mask_keep]
    vv_keep = vv[depth_mask_keep]
    if colored:
        rgb_keep = rgb[vv_keep, uu_keep]

    # 5. ç›¸æœºç³»â†’ä¸–ç•Œç³»
    pts_world = (R_cv @ pts_cam.T).T + t_cv[None, :]

    # 6. ä¸‹é‡‡æ ·
    if subsample_amt > 0:
        perm = np.random.permutation(len(pts_world))[::subsample_amt]
        pts_world = pts_world[perm]
        uu_keep = uu_keep[perm]
        vv_keep = vv_keep[perm]
        if colored:
            rgb_keep = rgb_keep[perm]

    # ç»„è£…ç»“æœ
    ret = {
        'worldpoints': pts_world,
        'pixcoords': np.stack([uu_keep, vv_keep], axis=1),
        'depth_image': depth_image,
        'screen_width': W,
        'screen_height': H,
        'c2w': c2w
    }
    if colored:
        ret['colors'] = rgb_keep
        ret['rgb_image'] = rgb_image
    return ret

# -------------------------- åˆå¹¶ä¸å¯è§†åŒ– --------------------------
def merge_clouds_world_points(clouds):
    if isinstance(clouds, dict):
        return clouds
    mergeable = ['worldpoints']
    if all(['colors' in cl for cl in clouds]):
        mergeable.append('colors')
    merged = {k: [] for k in mergeable}
    for cl in clouds:
        for k in mergeable:
            merged[k].append(cl[k])
    return {k: np.concatenate(v, axis=0) for k, v in merged.items()}

def visualize_clouds(clouds):
    import open3d
    if isinstance(clouds, dict):
        clouds = [clouds]
    o3dcloud = open3d.geometry.PointCloud()
    o3dcloud.points = open3d.utility.Vector3dVector(np.concatenate([c['worldpoints'] for c in clouds]))
    if 'colors' in clouds[0]:
        colors = []
        for c in clouds:
            if c['colors'].dtype == np.uint8:
                colors.append(np.float32(c['colors']) / 255.)
            else:
                colors.append(c['colors'])
        o3dcloud.colors = open3d.utility.Vector3dVector(np.concatenate(colors))
    open3d.visualization.draw([o3dcloud])


def add_camera_global_axis(merged_cloud, valid_clouds):
    # hyperparameter
    # N_global = 100 # number of points for global XYZ
    max_global = 100
    N_camera = 2000 # number of points for camera xyz
    max_camera = 10

    # visualize the global XYZ
    # global_x = np.zeros((N_global, 3))
    # global_x[:,0] = np.linspace(0, max_global, N_global)
    # global_x_color = np.zeros(global_x.shape)
    # global_x_color[:,0] = 1

    # global_y = np.zeros((N_global, 3))
    # global_y[:,1] = np.linspace(0, max_global, N_global)
    # global_y_color = np.zeros(global_y.shape)
    # global_y_color[:,1] = 1

    # global_z = np.zeros((N_global, 3))
    # global_z[:,2] = np.linspace(0, max_global, N_global)
    # global_z_color = np.zeros(global_z.shape)
    # global_z_color[:,2] = 1

    c2ws = []
    for valid_cloud in valid_clouds:
        c2ws.append(valid_cloud['c2w'][:3, :4])
    c2ws = np.array(c2ws)

    # visualize the camera xyz
    camera_centers = c2ws[:,:3,3]
    camera_centers_color = np.zeros(camera_centers.shape)

    camera_xs = np.linspace(0, max_camera, N_camera).reshape(N_camera, 1, 1)
    camera_x_dirs = c2ws[:,:3,0]
    camera_x_dirs = camera_x_dirs.reshape(1, *camera_x_dirs.shape)
    camera_xs = camera_xs * camera_x_dirs + camera_centers[None]
    camera_xs = camera_xs.reshape(-1, 3)
    camera_xs_color = np.zeros(camera_xs.shape)
    camera_xs_color[:,0] = 255

    camera_ys = np.linspace(0, max_camera, N_camera).reshape(N_camera, 1, 1)
    camera_y_dirs = c2ws[:,:3,1]
    camera_y_dirs = camera_y_dirs.reshape(1, *camera_y_dirs.shape)
    camera_ys = camera_ys * camera_y_dirs + camera_centers[None]
    camera_ys = camera_ys.reshape(-1, 3)
    camera_ys_color = np.zeros(camera_ys.shape)
    camera_ys_color[:,1] = 255

    camera_zs = np.linspace(0, max_camera, N_camera).reshape(N_camera, 1, 1)
    camera_z_dirs = c2ws[:,:3,2]
    camera_z_dirs = camera_z_dirs.reshape(1, *camera_z_dirs.shape)
    camera_zs = camera_zs * camera_z_dirs + camera_centers[None]
    camera_zs = camera_zs.reshape(-1, 3)
    camera_zs_color = np.zeros(camera_zs.shape)
    camera_zs_color[:,2] = 255

    # plots
    pts = np.concatenate([
        camera_centers,
        # global_x, global_y, global_z, 
        camera_xs, camera_ys, 
        camera_zs,
    ], axis=0)
    colors = np.concatenate([
        camera_centers_color,
        # global_x_color, global_y_color, global_z_color, 
        camera_xs_color, camera_ys_color, 
        camera_zs_color,
    ], axis=0).astype(np.uint8)
    merged_cloud['worldpoints'] = np.concatenate([merged_cloud['worldpoints'], pts])
    merged_cloud['colors'] = np.concatenate([merged_cloud['colors'], colors])

# -------------------------- ä¸»å‡½æ•° --------------------------
if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument("depth_files", nargs="+")
    parser.add_argument("-max", "--max_distance_clip_cloud", type=float, default=50.0)
    parser.add_argument("-ss", "--subsample_amt", type=int, default=3)
    parser.add_argument("-nc", "--no_color_avail", action="store_false", dest="color_avail")
    parser.add_argument("-scale", "--pose_scale", type=float, default=1.0, help="ç›¸æœºä½ç½®ç¼©æ”¾å› å­")
    parser.add_argument("-o", "--save_to_file", type=str, default="output.ply")
    args = parser.parse_args()

    args.depth_files = files_glob(args.depth_files)
    if not args.depth_files:
        print("âŒ æœªæ‰¾åˆ°æ·±åº¦æ–‡ä»¶")
        exit(1)

    raw_clouds = process_map(
        partial(load_cloud_via_meta,
                colored=args.color_avail,
                max_distance=args.max_distance_clip_cloud,
                subsample_amt=args.subsample_amt,
                pose_scale=args.pose_scale),
        args.depth_files
    )

    valid_clouds = [c for c in raw_clouds if c is not None]
    if len(valid_clouds) == 0:
        print("âŒ æœªåŠ è½½åˆ°æœ‰æ•ˆç‚¹äº‘")
        exit(1)

    print(f"âœ… åŠ è½½{len(valid_clouds)}å¸§æœ‰æ•ˆç‚¹äº‘ï¼Œåˆå¹¶ä¸­...")
    merged_cloud = merge_clouds_world_points(valid_clouds)
    if args.save_to_file:
        save_cloud_to_file(merged_cloud, args.save_to_file)
        print(f"ğŸ’¾ ç‚¹äº‘å·²ä¿å­˜è‡³: {args.save_to_file}")
    
    # add_camera_global_axis(merged_cloud, valid_clouds)
    
    
    visualize_clouds(merged_cloud)